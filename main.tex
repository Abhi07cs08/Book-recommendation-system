\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}

\title{\textbf{BookNerd Recommendation System}\\
\large CSC111 Final Project Report}
\author{Aadi Chauhan \and Abhinn Kaushik \and Dennis Bince \and Prabeer Sohal}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}

Recommender systems have become essential tools for helping users navigate large collections of items, such as books. Traditional book recommendation approaches often rely on numerical ratings or purchase history alone, which can miss the nuances of how a user felt about a book. User reviews contain rich sentiment information that can complement ratings to provide more personalized recommendations. This project aims to leverage both the structured rating data and unstructured review text to better capture user preferences. \textbf{The goal of this project is to develop a personalized book recommendation system that integrates sentiment analysis of review texts with user rating data to improve the relevance of recommended books.} By incorporating sentiment, we can distinguish between a lukewarm review and a genuinely enthusiastic one even if both have similar star ratings.

The motivation for this system stems from the desire to assist readers---especially those exploring independent ("indie") books---in discovering new titles that align with their tastes. Smaller or niche book communities may not have the benefit of massive datasets powering sophisticated recommendation engines, so a tailored approach that makes the most of available ratings and reviews is valuable. Our project, \textit{BookNerd Recommendation System}, provides an interactive platform where users can see recommendations and explore the underlying data relationships. We use graph data structures and algorithms (as taught in CSC111) to build a recommendation engine that is intuitive to use and offers insight into how recommendations are generated.

\section{Dataset Description}

The project utilizes a custom dataset stored in a CSV file named \texttt{sample\_dataset.csv}. This dataset was constructed specifically for the BookNerd system and contains four columns: \texttt{user\_id}, \texttt{book\_id}, \texttt{rating}, and \texttt{review}. Each row corresponds to a single user's review of a book. The \texttt{user\_id} is a unique identifier for the reviewer, and \texttt{book\_id} is a unique identifier for the book being reviewed. The \texttt{rating} is an integer score (on a 1 to 5 scale) that the user assigned to the book, and the \texttt{review} is a text field containing the user's written opinion about the book.

To incorporate textual sentiment into our recommendation algorithm, we compute a sentiment score for each review using TextBlob. TextBlob's sentiment analysis returns a polarity value between -1.0 (extremely negative sentiment) and +1.0 (extremely positive sentiment). We then normalize this polarity to a 0 to 1 range:
\[
\text{normalized\_sentiment} = \frac{\text{polarity} + 1}{2}.
\]
At the same time, we normalize the numerical \texttt{rating} to a 0 to 1 scale by dividing the rating by 5.0. Next, we blend these two signals into an edge weight:
\[
\text{edge\_weight} = 0.7 \times \text{normalized\_rating} \;+\; 0.3 \times \text{normalized\_sentiment}.
\]
If the dataset contained multiple entries for the same user--book pair, we average their weights. This step consolidates the dataset into a single "user--book" interaction graph, ready to be used in our final recommendation model.

\section{Computational Overview}

We model the data as a \textit{bipartite graph} with NetworkX, representing users on one side and books on the other. An edge connects a user to a book if the user reviewed that book, and each edge has a weight determined by combining rating and sentiment. This helps us see broad patterns at a glance, such as how many books a user has read (node degree) or how many reviewers a particular book had (another node degree).

In addition to the bipartite representation, we create a user--book matrix (via pandas) where rows are users and columns are books, populated by the edge weights. Using scikit-learn's \texttt{cosine\_similarity} function on this matrix, we compute a user--user similarity matrix. A higher cosine similarity score indicates that two users have a closely aligned distribution of preferences across the book set. This forms the basis for our user-based collaborative filtering approach.

\textbf{Recommendation Algorithm.} Given a target user, we:
\begin{enumerate}
    \item Identify the $K$ nearest neighbors (top-similarity users).
    \item Collect all books those neighbors liked, excluding those the target user already knows.
    \item Compute a recommendation score for each candidate book by summing (similarity $\times$ edge weight) across neighbors who rated that book.
    \item Rank the books by their scores and recommend the top $N$.
\end{enumerate}
Since edge weights incorporate sentiment, a book with glowing textual reviews weighs more than one with only moderate sentiment.

\subsection*{Visualizations and Core Libraries}

We rely on:
\begin{itemize}
    \item \textbf{pandas} for data loading and cleaning,
    \item \textbf{NumPy} for matrix operations,
    \item \textbf{NetworkX} for graph-based representations,
    \item \textbf{TextBlob} for extracting sentiment polarity,
    \item \textbf{scikit-learn} for cosine similarity,
    \item \textbf{Altair} for interactive charts,
    \item \textbf{matplotlib} as needed,
    \item \textbf{Streamlit} for the user interface.
\end{itemize}
Bar charts generated by Altair visualize the most popular books (by number of users), the top-rated books, overall rating distribution, and the most active users. The interface also allows showing advanced graph metrics, like betweenness centrality, to reveal influential nodes.

\section{Instructions for Running the Application}

\begin{enumerate}
\item \textbf{Install Dependencies:} Have Python 3.13 or later and run:
\begin{verbatim}
pip install -r requirements.txt
\end{verbatim}

\item \textbf{Prepare the Dataset:} Place \texttt{sample\_dataset.csv} in the same folder as \texttt{main.py}.

\item \textbf{Run Streamlit:} Type:
\begin{verbatim}
streamlit run main.py
\end{verbatim}
to launch the application locally.

\item \textbf{Interact with the App:}
    \begin{itemize}
        \item The sidebar has checkboxes to toggle charts and an option to display advanced graph metrics.
        \item In the main section, you can request recommendations in two ways:
        \begin{enumerate}
            \item \textbf{Get Recommendations (Existing User):} Enter a user ID and select how many neighbors ($K$) or how many books ($N$) to list. 
            \item \textbf{Custom Profile Recommendations:} Instead of picking an existing user, select a few books you like, and the system treats them as strong preferences for a new user.
        \end{enumerate}
        \item You may also enable "Show Recommendations for All Users" to check top suggestions across the dataset.
    \end{itemize}
\end{enumerate}

\section{Changes from Proposal}

Our final system closely reflects the original project proposal, with no major feature removals. The main addition is "Custom Profile Recommendations" to accommodate new users who can manually select some books they like, allowing immediate recommendations even without a prior history.

\section{Discussion}

Our approach of combining graph-based methods, sentiment analysis, and user-based collaborative filtering has yielded meaningful recommendations and a visually accessible interface. Users with rich review histories benefit from strong neighbor matches, as the cosine similarity matrix is better informed when users have multiple rated books, especially if they left detailed reviews with high or low sentiment. For example, in tests with sample user profiles, we observed that those who rated and reviewed at least five or six books generally saw more nuanced and fitting recommendations. The sentiment component provides extra nuance: a user who rates everything 3/5 might still produce strong edges when they write positively charged reviews, boosting certain books above others.

Despite these positive outcomes, several known constraints and limitations arise. First, the \textit{cold start problem} remains a challenge: a brand-new user with no reviews or ratings effectively has no connections in the bipartite graph, so the system has nothing to go on. Our “Custom Profile” feature partially mitigates this by letting new users pick some liked books upfront, but it is still manual and might not capture truly new readers with no familiarity of the book list. Second, the dataset itself is fairly small and may not fully represent the complexity of real-world user–book ecosystems. In a sparse dataset, many users share few (if any) overlapping books, reducing the effectiveness of collaborative filtering.

Another constraint is the reliability of the sentiment analysis. TextBlob is an accessible, straightforward library, but it can misinterpret nuanced language, sarcasm, or domain-specific expressions. Consequently, a user might intend to praise an aspect of a book while critiquing another, but the overall sentiment score may not reflect this mixed opinion accurately. The edge weight formula (70\% rating, 30\% sentiment) is a heuristic that works reasonably well but could be adjusted or replaced by a more adaptive strategy (e.g., learning an optimal blend through regression). Furthermore, if the dataset were scaled up to thousands of users and tens of thousands of books, computing a full user–user similarity matrix might become computationally expensive (costly in both time and memory). Techniques like approximate nearest neighbors, user clustering, or even switching to matrix factorization approaches could be explored to preserve performance while still capturing rating and sentiment patterns.

In practice, our results suggest that embedding textual sentiment in the rating matrix can reveal pockets of hidden similarity between users. For instance, two users might award an identical numeric rating to a particular title, but one user’s review might be highly enthusiastic, while the other’s is mildly positive. Our approach captures that difference, leading to a more finely tuned notion of “taste alignment.” Beyond the mechanical improvements, participants testing the system appreciated the interpretability of the results. Displaying bar charts of popular or top-rated books allowed them to contextualize the recommendations: if a recommended book is shown to be especially popular or highly rated, it is more reassuring that the suggestion stems from real community interest.

Another advantage of using a bipartite graph is that it can be extended with additional node types or attributes. For example, a future version of BookNerd might include authors or genres as separate nodes, creating a more complex multi-partite structure. Then, an edge connecting a book to a genre node could factor into the recommendation logic, helping connect users to books with matching tags, addressing both collaborative and content-based approaches. Implementing that sort of hybrid system is a natural next step to deal with new books or new users who only know their preferred genres. We could also improve the user experience by letting them specify certain constraints or preferences (like “I only want science fiction” or “avoid any thriller content”), which the system could respect by filtering books that do not match.

Looking forward, we envision several further enhancements. First, refining the sentiment analysis to handle advanced linguistic cues—perhaps by using a transformer-based model—could produce more accurate edge weights, especially for longer or more sophisticated reviews. Second, adjusting the ratio of rating to sentiment weighting (0.7 vs 0.3) dynamically per user might better reflect individual reviewing styles (some users rely heavily on star ratings, while others articulate their thoughts in detail). Finally, continuing to refine the UI to provide clarity about why specific books are recommended (explanatory “because user X had similar preferences” or “due to strong positive reviews from your neighbors”) would increase transparency and user trust. Overall, the current prototype demonstrates that sentiment-based weighting does indeed improve collaborative filtering for many users, and that representing user--book interactions with a weighted bipartite graph is both intuitive and powerful. As the system evolves, we anticipate that deeper integration of metadata and advanced sentiment analysis will yield still more personalized, robust, and explainable recommendations.

\section*{References}
\begin{thebibliography}{9}
\bibitem{TextBlob} TextBlob Documentation -- \textit{TextBlob: Simplified Text Processing} (Steven Loria, 2018). \url{https://textblob.readthedocs.io}
\bibitem{pandas} pandas Documentation -- \textit{pandas: Python Data Analysis Library} (Wes McKinney et al., 2022). \url{https://pandas.pydata.org/docs/}
\bibitem{NetworkX} NetworkX Documentation -- \textit{NetworkX: Network Analysis in Python} (Aric Hagberg et al., 2023). \url{https://networkx.org/documentation/stable/}
\bibitem{scikit-learn} scikit-learn Documentation -- \textit{Machine Learning in Python} (Pedregosa et al., 2011). \url{https://scikit-learn.org/stable/documentation.html}
\bibitem{Altair} Altair Documentation -- \textit{Declarative Visualization in Python} (Jake VanderPlas et al., 2022). \url{https://altair-viz.github.io/}
\bibitem{Matplotlib} Matplotlib Documentation -- \textit{Matplotlib: Visualization with Python} (John D. Hunter, 2007). \url{https://matplotlib.org/stable/}
\bibitem{Streamlit} Streamlit Documentation -- \textit{Turn data scripts into shareable web apps} (Streamlit Inc., 2023). \url{https://docs.streamlit.io/}
\end{thebibliography}

\end{document}
