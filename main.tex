\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{url}
\title{\textbf{BookNerd Recommendation System}\\\large CSC111 Phase 2 Final Project}
\author{Aadi Chauhan \and Abhinn Kaushik \and Dennis Bince \and Prabeer Sohal}
\date{\today}
\begin{document}
\maketitle

\section*{Project Title and Team Members}
\noindent \textbf{Project Title:} \textit{BookNerd Recommendation System} \\
\noindent \textbf{Team Members:} Aadi Chauhan, Abhinn Kaushik, Dennis Bince, Prabeer Sohal

\section{Introduction}
Recommender systems have become essential for helping users navigate the overwhelming number of choices in domains like movies, music, and books. In the context of books, readers often rely on reviews and ratings from others to decide what to read next. **How can we leverage graph theory and sentiment analysis to improve personalized book recommendations?** This is the central question that our project aims to answer. The motivation for \textit{BookNerd} is to build a recommendation system that not only uses collaborative filtering based on user ratings, but also incorporates the sentiment of user-written reviews to better capture user preferences. 

Traditional collaborative filtering recommends books to a user by finding other users with similar taste and suggesting books those similar users liked. However, numeric ratings alone may not fully reflect a reader’s opinion—textual reviews contain valuable sentiment information (positive or negative feelings) about the book. Our approach augments the rating data with sentiment analysis of reviews to create a more nuanced recommendation engine. We model the relationships between users and books as a graph structure, which firmly roots our project in graph theory. In this graph, users and books are represented as nodes, and edges encode interactions (ratings and reviews) between a user and a book. This graph-theoretic perspective allows us to apply network-based analysis to the recommendation problem and visualize the connectivity between users and books. Overall, the goal of \textit{BookNerd} is to demonstrate that incorporating review sentiment and graph-based techniques into a user-based collaborative filtering system can enhance the relevance and diversity of book recommendations.

\section{Dataset Description}
The dataset used for this project is a single CSV file (``\texttt{sample\_dataset.csv}'') containing user book reviews and ratings. It consists of four columns: \texttt{user\_id}, \texttt{rating\_id}, \texttt{rating}, and \texttt{review}. Each row corresponds to a single book review by a user. The \texttt{user\_id} is a unique identifier for the user, and the \texttt{rating\_id} serves as an identifier for the book being reviewed (in our context, this effectively identifies the book that the rating refers to). The \texttt{rating} column is a numerical score (for example, on a 1--5 scale) that the user gave to the book, and the \texttt{review} column contains the text of the user’s written review for that book.

This dataset was created for the purposes of the project (no external data source was used), so it is relatively limited in size. We made sure the data includes multiple reviews per user and per book to allow meaningful collaborative filtering. In preparing the data for analysis, we normalized the rating values to a common scale. This normalization (e.g., scaling ratings to the range 0 to 1) ensures that all users’ ratings are comparable and prevents biases from users who use only high or low ratings. We also leverage the \texttt{review} text: for each review, we perform sentiment analysis to extract a sentiment polarity score. We use TextBlob’s sentiment analyzer to compute a polarity value between $-1.0$ and $1.0$ for each review, where $1.0$ indicates a very positive sentiment and $-1.0$ a very negative sentiment. A positive review (e.g., praise for the book) yields a high polarity, while a critical or negative review yields a low (or negative) polarity. These polarity scores serve as an additional signal of user preference beyond the raw numeric rating. For instance, a user might give a moderately high rating (say 4/5) but write an unenthusiastic review; our system can detect the lukewarm sentiment and treat that user’s appreciation of the book as less strong than the rating alone would imply. By contrast, a glowing review text could reinforce or even slightly upscale the effect of a given numeric rating. We do not incorporate any external metadata (such as book genres or authors) in this dataset, focusing solely on the user–book interactions (ratings and review sentiments) recorded in the CSV.

\section{Computational Overview}
Our recommendation system uses a combination of graph theory, natural language processing, and classic collaborative filtering techniques. The core data structure of \textit{BookNerd} is a bipartite graph consisting of two types of nodes: user nodes and book nodes. We construct this graph using the interactions from the dataset. If a user $u$ has reviewed/rated a book $b$, we add an edge between node $u$ and node $b$. Each such edge is assigned a weight that represents the strength of the user’s affinity for that book. The weight is derived from the user’s rating and review sentiment for that book. In particular, we use the normalized rating and combine it with the TextBlob polarity score for the review. For example, if user $u$ gave book $b$ a rating of 4/5 and the review text polarity is $0.5$ (moderately positive), we compute a combined weight (the exact formula can be tuned; one simple approach is weight $= \text{normalized\_rating} \times (1 + \text{polarity})/2$ to slightly boost positive reviews and dampen negative ones). This means a very positive review can increase the effective weight of a high rating, while a very negative review can significantly reduce the weight (even if the numeric rating was somewhat high). By constructing the graph in this way, we ensure that the edge weights encapsulate both explicit rating and implicit sentiment preference. We use the \texttt{networkx} library to create and manage this graph structure in our program.

Once the bipartite user–book graph is built, the recommendation problem can be approached by finding users who are similar to the target user (the user for whom we want to recommend books). In graph terms, two users are likely to be similar if they share many connections to the same book nodes, especially if those connections (edges) have high weights. Rather than directly computing similarity from the graph structure, we implement a user-based collaborative filtering approach using vector representations of user preferences. We transform the data into a user–book matrix, where each row corresponds to a user and each column corresponds to a book. The entries of this matrix are the edge weights from the graph: entry $(u,b)$ is the weight representing user $u$’s interest in book $b$ (this entry is zero if $u$ has not rated book $b$). Each user $u$ is thus represented by a vector $w_{u}$ of weights across all books. We then compute user–user similarity using cosine similarity between these weight vectors. If $w_{u}$ and $w_{v}$ are the weight vectors for users $u$ and $v$, the cosine similarity is defined as: 

\[
\text{sim}(u,v) = \frac{\displaystyle\sum_{b \in B} w_{u,b} \cdot w_{v,b}}{\sqrt{\displaystyle\sum_{b \in B} w_{u,b}^2}\ \sqrt{\displaystyle\sum_{b \in B} w_{v,b}^2}}\,. 
\]

Here $B$ denotes the set of all books, and $w_{x,b}$ is the weight of the edge between user $x$ and book $b$ (which is 0 if no such edge exists). Cosine similarity yields a score between $-1$ and $1$ (though in our case, weights are nonnegative or mildly adjusted by sentiment, so similarities will fall between 0 and 1). A value of 1 indicates that two users have identical preference vectors (highly similar tastes), while 0 indicates no overlap in books or completely uncorrelated preferences. We use the \texttt{numpy} library to efficiently perform these vector computations and calculate similarity scores for all pairs of users. In practice, for generating recommendations, we don’t need all user–user similarities at once; we compute the similarities between a target user and every other user.

With similarity measures in hand, the recommendation algorithm proceeds as follows. Given a target user (for example, the user currently interacting with the BookNerd app), we identify the top $k$ other users with highest cosine similarity to the target. These $k$ users can be thought of as the “nearest neighbors” in terms of taste. We then aggregate the books that these similar users have highly rated (and reviewed positively) which the target user has not yet read. For each candidate book among these, we can compute a recommendation score—one common approach is to take a weighted sum of the neighbors’ ratings for that book, weighted by the similarity of each neighbor to the target user. Books that the target user has already rated are excluded from recommendations. Finally, we rank these candidate books by their score and select the top $N$ books as the recommended set for the target user. This ensures that the recommendations come predominantly from users who have a high affinity to the target user, making it likely that the target user will enjoy those books. The incorporation of sentiment in the weights adds an extra layer: a book that a neighbor user rated somewhat well but raved about in text might receive a higher weight in our system, and thus could surface in recommendations, increasing the diversity of suggestions beyond what pure rating-based collaborative filtering might yield.

Apart from the core recommendation logic, our program includes components for data processing and visualization. We use the \texttt{pandas} library [2] to read the CSV dataset into a DataFrame and to perform initial data cleaning and transformation. For instance, using pandas we group and filter the data as needed, create the user–book pivot table for the rating matrix, and apply normalization to rating values. We also use pandas to compute some summary statistics from the data (such as the number of reviews per book or per user, or the average rating of each book) which are useful for the visualization part of our application. The sentiment analysis is done with TextBlob [1] – for each review string, we create a \texttt{TextBlob} object and retrieve its \texttt{sentiment.polarity} value. This polarity value is then stored or merged into the DataFrame alongside the numeric rating.

Using the \texttt{networkx} library [3], we construct the bipartite graph as described: each user and each book is added as a node (we label book nodes differently or use a prefix to distinguish them from user nodes, since it’s a single graph). Then for each review entry, we add an edge with the appropriate weight. NetworkX provides convenient data structures for graphs, and although our use-case (collaborative filtering) doesn’t require complex graph algorithms beyond building the graph, representing the data as a graph made it easy to visualize and reason about the relationships. For instance, one can query NetworkX for the neighbors of a user (which returns all books that user has rated) or the neighbors of a book (all users who rated that book). We also utilize NetworkX’s drawing capabilities in combination with \texttt{matplotlib} to create a network layout diagram. Specifically, we generate a subgraph of the overall network focusing on a particular user and their immediate connections (and possibly connections of those books to other users) and draw this graph. We use a layout algorithm (such as NetworkX’s spring layout) to position the nodes, and then plot user nodes and book nodes with different colors or shapes to differentiate them. Edge thickness or transparency can reflect the weight (strong likes vs. weak likes). This graph visualization helps illustrate why certain recommendations are made (e.g., it might show that the target user is connected to a book via another user who loved that book).

For interactive visualizations and the user interface, we chose \texttt{Streamlit} to build a web application for our system. Streamlit allows us to create a clean sidebar and main content area easily. In the sidebar of the app, we provide user controls such as selecting a target user ID for whom to generate recommendations. This is done via a dropdown menu listing all available \texttt{user\_id}s in the dataset. We also include toggles (checkboxes) in the sidebar that let the user adjust certain options: for example, one toggle allows the user to switch the recommendation mode between “use sentiment-enhanced weights” and “use ratings only.” This effectively lets the user (or the TA testing the program) see the difference sentiment analysis makes in the recommendations. Another checkbox allows the user to show or hide the network graph visualization, in case they want to focus only on the list of recommended books. We also include a slider to adjust the number of similar users ($k$) considered or the number of recommendations to display, providing a way to explore different neighborhood sizes.

The main section of the Streamlit app displays the outputs and visualizations. At the top, we present an overview chart of popular books. We created this visualization using \texttt{Altair}, a declarative visualization library for Python [4]. The chart (an Altair bar chart) shows, for instance, the top 10 books in the dataset by popularity. Popularity can be defined as the number of users who rated the book, or by the average rating. In our case we defined it by the number of distinct users who reviewed the book (since our dataset is small, we chose count of reviews as a measure of popularity). This bar chart is interactive in that it has tooltips (when you hover over a bar, you can see the exact number of reviews and the average rating for that book). The chart provides context about the dataset – e.g., it might show that certain books have been reviewed by many users (hence likely well-known books) while others are more niche. Below the popular books chart, when a target user is selected from the sidebar, the application displays the personalized recommendations for that user. The recommendations are listed with the book title (or ID) and optionally some indication of why it was recommended (for example, “Recommended because User 42 (85\% similar) rated it 5/5”). We show typically the top 5 recommended books. Alongside this list, if the user enabled the network graph toggle, we display the network visualization generated via NetworkX/Matplotlib focusing on the target user. This graph plot is rendered as an image within the Streamlit app (Streamlit can display Matplotlib figures directly). The graph helps the user see the connections: for example, the target user node connected to a few book nodes, and another user node who is similar connected to that same book node and perhaps other books that the target hasn’t read yet. This essentially highlights the reasoning of the collaborative filter. We label nodes or provide a legend so that the viewer can distinguish which node is the target user, which nodes are “neighbor” users, and which are books (candidates for recommendation). The interface is dynamic: if a different user is selected, or if the sentiment toggle is switched, the recommendations and graphs update immediately (Streamlit reruns the script and updates outputs on any widget change).

In summary, our computational approach involves building a weighted bipartite graph of users and books using the dataset, analyzing review text sentiment to refine those weights, calculating user similarity with cosine similarity (using linear algebra operations via NumPy), and finally producing recommendations based on nearest neighbors in the graph. The results are delivered through an interactive Streamlit web application, with visual support from charts (via Altair) and graphs (via NetworkX/Matplotlib). Key libraries and technologies used include: \texttt{pandas} for data handling, \texttt{TextBlob} for NLP sentiment analysis, \texttt{numpy} for numeric computations, \texttt{networkx} for graph representation, \texttt{matplotlib} for plotting, \texttt{Altair} for interactive charts, and \texttt{Streamlit} for the overall user interface.

\section{Instructions to Run the Program}
To run the BookNerd recommendation system, please ensure you have **Python 3.13** installed on your system (the program was developed and tested with Python 3.13). You will also need to install the required Python packages listed in the provided \texttt{requirements.txt} file. These include Streamlit, pandas, numpy, TextBlob, networkx, matplotlib, Altair, and other dependencies. You can install all dependencies by running the command:

\begin{verbatim}
pip install -r requirements.txt
\end{verbatim}

in your terminal. This will fetch and install all necessary libraries. (No special installation steps beyond this are required; all libraries are available via PyPI. In particular, installing TextBlob via \texttt{pip} will also install its necessary NLP corpora for sentiment analysis by default.)

Next, ensure that the dataset file \texttt{sample\_dataset.csv} is present in the same directory as the program files. (Since this dataset is included with our submission, the TA should not need to download it separately. If for any reason the file is not present, it should be obtained from the submitted project zip archive and placed in the project folder.) The program expects this file name exactly and will attempt to read it on startup.

Once the environment is set up, the program can be launched by running the file \texttt{main.py}. Because this project uses Streamlit for the user interface, you should run it with Streamlit. In a command prompt or terminal, navigate to the project directory (where \texttt{main.py} is located) and execute:

\begin{verbatim}
streamlit run main.py
\end{verbatim}

This command will start the Streamlit server and open the BookNerd web application in your default web browser (it usually opens at \texttt{localhost:8501}). 

After the app loads, you will see the **BookNerd Recommendation System** interface. On the left side is the Streamlit sidebar, which contains interactive controls. At the top of the sidebar, there is a dropdown menu labeled e.g. "Select User" allowing you to choose a user ID from the dataset. To test the recommendations, select any user (by their ID). Below the user selector, there are a few checkboxes that toggle features:
\begin{itemize}
  \item \textbf{Include Sentiment Analysis:} if checked (the default is checked), the system will use the sentiment-weighted graph for recommendations. If you uncheck this, the recommendations will be generated using only the numerical ratings (ignoring review sentiment). This allows you to compare the outcomes with and without sentiment.
  \item \textbf{Show Network Graph:} if checked, the app will display the graph visualization of the selected user’s connections and similar users. If you prefer not to see the graph (or to speed up the interface), you can uncheck this.
  \item \textbf{Show Popular Books Chart:} this toggle controls the display of the popular books bar chart on the main page. (By default, this may be on to show an overview of the dataset’s popular books.)
\end{itemize}
There may also be a slider to adjust the number of recommendations (for example, 5, 10, 15) to display, and similarly a slider for the number of similar users ($k$) to consider in the collaborative filtering. These controls are all in the sidebar for easy access.

On the right side (the main page), the output visuals and results are displayed. At the top, you will see a title and a brief description of the project. Below that, if the "Show Popular Books Chart" option is enabled, a bar chart will be shown that lists the most popular books in the dataset. Each bar corresponds to a book and is proportional to the number of users who reviewed that book. You can hover your mouse over the bars to see details (the exact number of reviews and the average rating for that book, thanks to Altair’s interactive tooltip feature).

Under the popularity chart (or directly under the description if the chart is hidden), the personalized recommendation results are displayed once you have selected a user from the sidebar. The app will list the recommended books for the chosen user. Each recommendation might be displayed as a bullet point or a numbered list, typically showing the book’s title or ID. In our implementation, if the dataset includes book titles, we display the title; if only IDs are present, we show the ID (e.g., "Book 27"). Alongside each recommended item, we provide a brief explanation such as "(recommended based on similar users’ ratings)" or even list a particularly similar user who enjoyed it, to give insight into the recommendation. If the network graph option is enabled, the graph will appear below or next to the recommendation list (Streamlit typically stacks elements vertically, but we ensured the layout is clean). The graph image will have the target user highlighted (for example, with a distinct color or label), and books recommended might be circled or colored differently as well. Users in the graph who influenced the recommendation (the similar neighbors) are also shown.

You can try selecting different users to see how the recommendations change. The interface will update in real-time. For example, selecting a user with a lot of reviews will likely show a rich graph and a strong set of recommendations, whereas a user with only one or two reviews might result in fewer or more generic recommendations. If you toggle off "Include Sentiment Analysis," you can observe if any recommended books change for a given user—any differences might illustrate the effect of sentiment. Typically, with sentiment enabled, the system might favor books that had very positive textual reviews, whereas without sentiment it might lean purely on rating values.

In summary, to run the program: (1) install the required libraries, (2) ensure the data file is available, (3) run \texttt{streamlit run main.py}, and then use the web interface to select a user and view recommendations. The Streamlit app is designed to be largely self-explanatory, with section headers and captions to guide the user. If any issues arise (for instance, a library not installed or an error reading the CSV), those will appear in the terminal or browser; otherwise, the TA should be able to interact with the app smoothly following these instructions.

\section{Changes from Proposal}
Our final project implementation remained very faithful to the original proposal. We did not make any major changes to the project’s problem definition, methodology, or scope. The idea from the proposal—to use a graph-based collaborative filtering approach enhanced by sentiment analysis—was carried through to completion without significant deviation. All core components (building a user–book graph, computing similarities, using TextBlob for sentiment, and developing a Streamlit app for visualization) were implemented as planned. Minor adjustments were made in terms of parameter tuning and interface details (for example, we added the toggle to disable sentiment analysis as a feature for evaluation purposes, which was not explicitly mentioned in the proposal), but these do not constitute a change in direction, only an enhancement for better analysis. We also stayed within the project theme of applying graph theory to a real-world domain (recommendation systems) as initially intended. In summary, our project progressed according to the proposal’s plan, and the final product aligns closely with what we promised, with no major changes or scope reductions.

\section{Discussion}
The BookNerd recommendation system demonstrates the potential benefits of integrating sentiment analysis with graph-based collaborative filtering. Overall, the results we observed from our system support the idea that review sentiments can enrich recommendation outputs. When we tested the system with various users in the dataset, we found that it performs best for users who are well-connected in the user–book graph, meaning users who have rated a decent number of books and thus have more opportunity to share overlaps with others. For these users, the top similar neighbors identified by our cosine similarity measure tended to be intuitively reasonable (for example, we often saw that two users who both highly rated a particular series or author ended up with a high similarity score). The recommendations generated for such users were generally relevant: the system would suggest books that belong to genres or series the user has shown interest in. This indicates that the collaborative filtering component is functioning as expected — if user A and user B have a lot of books in common that they both liked, and B liked another book that A hasn’t read, that book is a good candidate for A. 

Importantly, we noticed that incorporating sentiment weighting made a subtle but meaningful difference in the recommendations. In cases where a user’s numeric ratings alone might paint an incomplete picture, the sentiment analysis provided additional insight. For example, one user in our dataset gave a rating of 3/5 to a book but wrote a very positive review commenting that they “really enjoyed the character development despite some slow parts.” Another user with similar tastes might have rated that same book a 5/5. In a pure rating-based system, the first user’s lukewarm numeric rating might de-emphasize that book in connecting the two users. However, BookNerd’s sentiment-aware approach recognized that the first user’s sentiment was actually quite positive (TextBlob polarity, say, +0.7) despite the moderate rating. As a result, the weight for that user-book edge was adjusted upward enough that those two users still appeared as a good match, and other books the second user loved were successfully recommended to the first. In contrast, when we toggled sentiment off in the system, we found that in a few cases the overlap between certain users seemed weaker and some recommendations changed, sometimes dropping a book that had positive text feedback but lower rating. Thus, the sentiment analysis helped capture scenarios of under-rated books or over-rated books by interpreting the review context, and this led to a more diverse set of recommendations. By diverse, we mean the recommendations were not always the highest-rated popular books; some were niche books that elicited enthusiasm in text reviews by like-minded readers. This is a notable advantage—our system can recommend a book that didn’t get the highest star rating on average but has a small fanbase that expressed strong love for it, aligning with a specific user’s interests.

Using graph analysis also brings some insights. We can consider graph-based metrics such as the degree of nodes or connectivity. In our visualization of the user–book network, popular books appear as hubs (book nodes with many user edges). These hub books often serve as common grounds linking users; as a result, they frequently appear in recommendations (especially as a safe bet for new or less-connected users). While recommending only the most popular books is not personalized, our system balances this by focusing on the nearest neighbors’ preferences. We observed that for a sparsely connected user (one who has rated very few books), the system might fall back to recommending some of the globally popular books or top average-rated books from the dataset. This is a reasonable fallback and is essentially the system handling the \textit{cold start} problem: with little information on a new user, the best one can do is suggest generally well-received books. However, this also highlights a limitation — if a user has no reviews in the system (a true cold start), our collaborative approach cannot personalize recommendations at all. In such a case, everyone would get roughly the same list of popular books. This is a known limitation of collaborative filtering systems; they rely on input data about a user’s preferences. Addressing cold start would require incorporating other techniques (like asking the new user to rate a few starter books or using content-based filtering with book metadata) which we noted as future work.

Another limitation of our current system is the scale of the dataset. Because our dataset is relatively small (by design, given the scope of the project), the recommendations and the user-user similarities are derived from limited information. In a small dataset, it’s possible that many users are only loosely connected, and the recommendations might sometimes be obvious (e.g., recommending the single book that two users have in common). In a larger, real-world dataset (like thousands of users and books), we would expect the algorithm to have more to work with and potentially yield more nuanced recommendations. The performance (speed) with our approach is perfectly fine for the small dataset (computing cosine similarities for a few dozen users is instantaneous, and the graph has no performance issues). For larger data, we might need to optimize certain steps (for example, computing similarities only locally or using approximate neighbor search if users are in the millions), but the algorithms we chose are fundamentally scalable with the right optimizations (pandas and numpy can handle quite large matrices in memory, and more advanced techniques or libraries like scikit-learn could be integrated if needed).

One interesting observation from testing the system is that the sentiment weighting can improve diversity of recommendations, as mentioned, but it also could potentially introduce noise if a sentiment analyzer misjudges a review’s tone. TextBlob generally performed well on our review texts (which were fairly clear in sentiment), but sentiment analysis is not perfect. In an edge case, sarcasm or subtle writing could fool the polarity score. This didn’t significantly affect our results, but it’s a consideration that relying on automatic sentiment analysis has some uncertainty.

In terms of meeting our project goal, the results indicate that our system does harness both graphs and NLP to create an enhanced recommendation experience. The recommendations do help answer our research question: by seeing different suggestions with sentiment on vs off, we gather evidence that sentiment analysis adds value in highlighting books that pure rating-based filtering might overlook. We also achieved an interactive visualization that aids in interpreting the results. The graph visualization in particular helped us (and would help the user/TA) verify why a certain book was recommended (you can literally see the chain: User A –(liked)→ Book X ←(liked)– User B, and User B –(liked)→ Book Y, thus Book Y is recommended to A). This interpretability is a strength of using graph-based methods.

For further improvement, there are several next steps we envision. First, expanding the user interface could make the system more user-friendly and informative. Currently, the Streamlit app is straightforward, but we could add features like a search bar to look up specific books or users, or allow the user to input a new review and get immediate recommendations (thereby simulating adding a new user or a new rating). We could also enrich the UI with more statistics (for instance, showing a user’s top genres if genre data were available, or letting the user filter recommendations by genre or year). This brings us to the second point: incorporating book metadata such as genres, authors, or publication year could greatly enhance the recommendations. For example, a user might want only science fiction book recommendations. If we had genre tags, we could easily add a content-based filter on top of the collaborative filter (only recommend books in the user’s favorite genre). Genre information could also help alleviate the cold start problem by matching new users to books via stated genre preferences. In a future iteration, we would like to integrate such content features, effectively creating a hybrid recommendation system.

Third, testing and deploying the system on a larger, real-world dataset would be an excellent next step. We would take a dataset from a source like Goodreads or Amazon book reviews (if available) with thousands of users and books. This would allow us to evaluate the scalability and performance of our approach, and also measure the quality of recommendations more objectively (for instance, by holding out some user ratings as a test set and seeing if the system predicts them). A larger dataset might also reveal new insights, such as the need for dimensionality reduction or more advanced similarity measures if data is very sparse. Additionally, with more data we could experiment with tuning the sentiment weight formula or thresholding (perhaps ignoring very low sentiment reviews altogether or something along those lines).

In conclusion, the BookNerd recommendation system successfully demonstrates an application of graph theory and sentiment analysis in a collaborative filtering context. The discussion above highlights that the system is particularly effective when ample data is available for a user, that sentiment analysis does improve the nuance of recommendations, and that while the approach has limitations (notably cold start and reliance on sufficient data), these can be addressed in future work. We believe that our project provides a solid foundation that could be built upon to create a fully-fledged personalized book recommendation platform that leverages both the collective intelligence of reader communities (through the user-user graph) and the rich information in their reviews (through NLP).

\section*{References}
\begin{enumerate}
    \item \textbf{TextBlob Documentation:} S. Loria et al., \textit{TextBlob: Simplified Text Processing} (Version 0.19.0) [Online]. Available: \url{https://textblob.readthedocs.io/}
    \item \textbf{pandas Documentation:} pandas Development Team, \textit{pandas User Guide} (Version 1.x) [Online]. Available: \url{https://pandas.pydata.org/docs/}
    \item \textbf{NetworkX Documentation:} A. A. Hagberg, D. A. Schult, & P. J. Swart, \textit{NetworkX Reference} (NetworkX 3.x) [Online]. Available: \url{https://networkx.org/documentation/}
    \item \textbf{Altair Documentation:} Altair Developers, \textit{Altair: Declarative Visualization in Python} (Version 5.x) [Online]. Available: \url{https://altair-viz.github.io/}
\end{enumerate}

\end{document}
